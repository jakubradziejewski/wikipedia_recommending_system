# Wikipedia Article Content Recommender System

[![Python](https://img.shields.io/badge/Language-Python%203.x-blue.svg)]()
[![Scraping](https://img.shields.io/badge/Tool-Scrapy-darkgreen.svg)]()
[![Model](https://img.shields.io/badge/Model-Content--Based%20(TF--IDF)-red.svg)]()

## Project Summary

This project implements a Content-Based Filtering system designed to recommend Wikipedia articles. The system includes an entire operational pipeline: data collection, text preparation, model building (TF-IDF), similarity calculation, and analysis of recommendation performance and interpretability.

Recommendations are generated by computing the **Cosine Similarity** between the TF-IDF vector of a query article (or the average vector of multiple articles) and all other articles in the corpus.

## Implementation Pipeline and Modules

The project's core logic is organized into three distinct phases within the `src/` directory.

### Phase 1: Data Acquisition and Preprocessing

| Module | Functionality | Key Technology |
| :--- | :--- | :--- |
| **`wikipedia_spider.py`** | Scrapes article text from Wikipedia using a **Breadth-First Search (BFS)** link traversal. Configured to collect up to 5000 articles starting from a seed page. | **Scrapy** |
| **`text_processor.py`** | Handles all NLP tasks: **tokenization, stopword removal, stemming (Porter), and lemmatization (WordNet)** with POS tagging. | **NLTK** |
| **`pipelines.py`** | Processes scraped items by applying the text processor and then exporting the data frame to a **Parquet file** (`data/wikipedia_articles.parquet`) for efficient storage. | **Scrapy Pipelines, pyarrow** |

### Phase 2: Similarity Engine and Modeling

| Module | Functionality | Key Technology |
| :--- | :--- | :--- |
| **`similarity_engine.py`** | Loads article data and builds the **TF-IDF Vectorizer** based on the lemmatized text column. Calculates **Cosine Similarity** to return ranked recommendations. | **scikit-learn** |

### Phase 3: Analysis and Evaluation

| Module | Functionality | Key Technology |
| :--- | :--- | :--- |
| **`statistics.py`** | Generates corpus statistics, including matrix density, feature counts, article text length metrics, and visualization of the **corpus-wide pairwise similarity distribution**. | **Pandas, scikit-learn** |
| **`strategy.py`** | Defines and compares different recommendation strategies (e.g., Random, Similar, Recursive) and calculates performance metrics like **internal coherence**. | **scikit-learn** |
| **`explainability.py`** | Calculates and plots the **element-wise contribution** of individual TF-IDF terms to the final similarity score, allowing for interpretation of the recommendation rationale. | **numpy, Matplotlib** |
