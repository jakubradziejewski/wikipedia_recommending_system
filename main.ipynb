{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3afb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:49:26 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: scrapybot)\n",
      "2025-11-07 14:49:26 [scrapy.utils.log] INFO: Versions:\n",
      "{'lxml': '6.0.2',\n",
      " 'libxml2': '2.14.6',\n",
      " 'cssselect': '1.3.0',\n",
      " 'parsel': '1.10.0',\n",
      " 'w3lib': '2.3.1',\n",
      " 'Twisted': '25.5.0',\n",
      " 'Python': '3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]',\n",
      " 'pyOpenSSL': '25.3.0 (OpenSSL 3.5.4 30 Sep 2025)',\n",
      " 'cryptography': '46.0.3',\n",
      " 'Platform': 'Linux-6.14.0-33-generic-x86_64-with-glibc2.39'}\n",
      "2025-11-07 14:49:26 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-11-07 14:49:26 [scrapy.extensions.telnet] INFO: Telnet Password: f74febe7d424450a\n",
      "2025-11-07 14:49:26 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-11-07 14:49:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'CONCURRENT_REQUESTS': 1,\n",
      " 'COOKIES_ENABLED': False,\n",
      " 'DEPTH_PRIORITY': 1,\n",
      " 'DOWNLOAD_DELAY': 0.5,\n",
      " 'LOG_LEVEL': 'INFO',\n",
      " 'RETRY_TIMES': 3,\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue',\n",
      " 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue',\n",
      " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Wikipedia BFS Crawler\n",
      "============================================================\n",
      "Starting from: https://en.wikipedia.org/wiki/Madagascar\n",
      "Target pages: 1000\n",
      "Strategy: BFS with 10 random links per page\n",
      "Estimated time: 10-15 minutes\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:49:26 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-11-07 14:49:26 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
      " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-11-07 14:49:26 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 261\u001b[39m\n\u001b[32m    257\u001b[39m     process.start()\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[43mrun_spider\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 257\u001b[39m, in \u001b[36mrun_spider\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    251\u001b[39m process = CrawlerProcess(settings={\n\u001b[32m    252\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mFEEDS\u001b[39m\u001b[33m'\u001b[39m: {},\n\u001b[32m    253\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLOG_LEVEL\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mINFO\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    254\u001b[39m })\n\u001b[32m    256\u001b[39m process.crawl(WikipediaSpider)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/defaultenv/lib/python3.12/site-packages/scrapy/crawler.py:502\u001b[39m, in \u001b[36mCrawlerProcess.start\u001b[39m\u001b[34m(self, stop_after_crawl, install_signal_handlers)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[32m    499\u001b[39m     reactor.addSystemEventTrigger(\n\u001b[32m    500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mafter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstartup\u001b[39m\u001b[33m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m._signal_shutdown\n\u001b[32m    501\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m \u001b[43mreactor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/defaultenv/lib/python3.12/site-packages/twisted/internet/asyncioreactor.py:253\u001b[39m, in \u001b[36mAsyncioSelectorReactor.run\u001b[39m\u001b[34m(self, installSignalHandlers)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.startRunning(installSignalHandlers=installSignalHandlers)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncioEventloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_forever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._justStopped:\n\u001b[32m    255\u001b[39m         \u001b[38;5;28mself\u001b[39m._justStopped = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/base_events.py:630\u001b[39m, in \u001b[36mBaseEventLoop.run_forever\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run until stop() is called.\"\"\"\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28mself\u001b[39m._set_coroutine_origin_tracking(\u001b[38;5;28mself\u001b[39m._debug)\n\u001b[32m    633\u001b[39m old_agen_hooks = sys.get_asyncgen_hooks()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/base_events.py:622\u001b[39m, in \u001b[36mBaseEventLoop._check_running\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_running():\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThis event loop is already running\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    624\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    625\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mCannot run the event loop while another loop is running\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: This event loop is already running"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:49:26 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-11-07 14:49:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-11-07 14:49:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-11-07 14:50:00 [wikipedia_spider] INFO: Progress: 50/1000 pages scraped\n",
      "2025-11-07 14:50:26 [scrapy.extensions.logstats] INFO: Crawled 94 pages (at 94 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-11-07 14:50:31 [wikipedia_spider] INFO: Progress: 100/1000 pages scraped\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "# NLTK imports for text processing\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Handles text preprocessing including tokenization, stemming, and lemmatization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Remove unwanted characters and normalize text\"\"\"\n",
    "        # Remove references like [1], [2], etc.\n",
    "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"\n",
    "        Process text: tokenize, remove stopwords, and apply stemming/lemmatization\n",
    "        Returns: dict with original, tokens, stems, and lemmas\n",
    "        \"\"\"\n",
    "        # Clean the text first\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(cleaned_text.lower())\n",
    "        \n",
    "        # Filter: keep only alphabetic tokens that are not stopwords\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens \n",
    "            if token.isalpha() and token not in self.stop_words and len(token) > 2\n",
    "        ]\n",
    "        \n",
    "        # Apply stemming\n",
    "        stems = [self.porter.stem(token) for token in filtered_tokens]\n",
    "        \n",
    "        # Apply lemmatization\n",
    "        lemmas = [self.lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens]\n",
    "        \n",
    "        return {\n",
    "            'original_text': cleaned_text,\n",
    "            'tokens': filtered_tokens,\n",
    "            'stems': stems,\n",
    "            'lemmas': lemmas,\n",
    "            'token_count': len(filtered_tokens)\n",
    "        }\n",
    "\n",
    "\n",
    "class WikipediaSpider(scrapy.Spider):\n",
    "    \"\"\"Spider for crawling Wikipedia pages with BFS strategy\"\"\"\n",
    "    \n",
    "    name = 'wikipedia_spider'\n",
    "    allowed_domains = ['en.wikipedia.org']\n",
    "    start_urls = ['https://en.wikipedia.org/wiki/Madagascar']\n",
    "    \n",
    "    # Custom settings\n",
    "    custom_settings = {\n",
    "        'ROBOTSTXT_OBEY': True,\n",
    "        'CONCURRENT_REQUESTS': 1,\n",
    "        'DOWNLOAD_DELAY': 0.5,\n",
    "        'COOKIES_ENABLED': False,\n",
    "        'RETRY_TIMES': 3,\n",
    "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'DEPTH_PRIORITY': 1,\n",
    "        'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue',\n",
    "        'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(WikipediaSpider, self).__init__(*args, **kwargs)\n",
    "        self.visited_urls = set()\n",
    "        self.max_pages = 100\n",
    "        self.pages_scraped = 0\n",
    "        self.data = []\n",
    "        self.text_processor = TextProcessor()\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \"\"\"Parse Wikipedia page and extract content\"\"\"\n",
    "        \n",
    "        # Check if we've reached the limit\n",
    "        if self.pages_scraped >= self.max_pages:\n",
    "            self.logger.info(f\"Reached maximum pages ({self.max_pages}), stopping\")\n",
    "            return\n",
    "        \n",
    "        url = response.url\n",
    "        \n",
    "        # Skip if already visited\n",
    "        if url in self.visited_urls:\n",
    "            return\n",
    "        \n",
    "        self.visited_urls.add(url)\n",
    "        self.pages_scraped += 1\n",
    "        \n",
    "        # Extract title\n",
    "        title = response.css('h1.firstHeading::text').get()\n",
    "        if not title:\n",
    "            title = response.url.split('/')[-1].replace('_', ' ')\n",
    "        \n",
    "        # Extract main content including text inside links\n",
    "        # Get all text from paragraphs, including linked text\n",
    "        paragraphs = response.css('#mw-content-text .mw-parser-output > p')\n",
    "        \n",
    "        content_parts = []\n",
    "        for para in paragraphs:\n",
    "            # Extract all text including text inside <a> tags\n",
    "            para_text = para.css('::text').getall()\n",
    "            content_parts.append(' '.join(para_text))\n",
    "        \n",
    "        content = ' '.join(content_parts)\n",
    "        \n",
    "        # If no paragraphs found, try alternative selector\n",
    "        if not content:\n",
    "            paragraphs_alt = response.css('#mw-content-text p')\n",
    "            content_parts = []\n",
    "            for para in paragraphs_alt:\n",
    "                para_text = para.css('::text').getall()\n",
    "                content_parts.append(' '.join(para_text))\n",
    "            content = ' '.join(content_parts)\n",
    "        \n",
    "        # Process the text\n",
    "        if content.strip():\n",
    "            processed = self.text_processor.process_text(content)\n",
    "            \n",
    "            # Store the data\n",
    "            self.data.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'original_text': processed['original_text'],\n",
    "                'tokens': ' '.join(processed['tokens']),\n",
    "                'stems': ' '.join(processed['stems']),\n",
    "                'lemmas': ' '.join(processed['lemmas']),\n",
    "                'token_count': processed['token_count'],\n",
    "                'text_length': len(processed['original_text'])\n",
    "            })\n",
    "            \n",
    "            if self.pages_scraped % 50 == 0:\n",
    "                self.logger.info(f\"Progress: {self.pages_scraped}/{self.max_pages} pages scraped\")\n",
    "        \n",
    "        # Extract links for BFS - only if we haven't reached the limit\n",
    "        if self.pages_scraped < self.max_pages:\n",
    "            # Extract all links from the content area\n",
    "            all_links = response.css('#mw-content-text a::attr(href)').getall()\n",
    "            \n",
    "            # Filter to get only valid Wikipedia article links\n",
    "            valid_links = []\n",
    "            for link in all_links:\n",
    "                # Must start with /wiki/\n",
    "                if not link.startswith('/wiki/'):\n",
    "                    continue\n",
    "                \n",
    "                # Skip special pages\n",
    "                if any(skip in link for skip in [':', 'Main_Page', '#']):\n",
    "                    continue\n",
    "                \n",
    "                # Convert to absolute URL\n",
    "                full_url = response.urljoin(link)\n",
    "                \n",
    "                # Skip if already visited\n",
    "                if full_url not in self.visited_urls:\n",
    "                    valid_links.append(full_url)\n",
    "            \n",
    "            # Remove duplicates\n",
    "            valid_links = list(set(valid_links))\n",
    "            \n",
    "            # Shuffle and take only 10 links\n",
    "            random.shuffle(valid_links)\n",
    "            selected_urls = valid_links[:10]\n",
    "            \n",
    "            self.logger.debug(f\"Found {len(valid_links)} valid links, selected {len(selected_urls)}\")\n",
    "            \n",
    "            # Yield requests for selected URLs\n",
    "            for new_url in selected_urls:\n",
    "                yield scrapy.Request(\n",
    "                    new_url, \n",
    "                    callback=self.parse,\n",
    "                    priority=0,\n",
    "                    errback=self.handle_error\n",
    "                )\n",
    "    \n",
    "    def handle_error(self, failure):\n",
    "        \"\"\"Handle request errors\"\"\"\n",
    "        self.logger.error(f\"Request failed: {failure.request.url}\")\n",
    "    \n",
    "    def closed(self, reason):\n",
    "        \"\"\"Called when spider is closed - save data to parquet\"\"\"\n",
    "        if self.data:\n",
    "            df = pd.DataFrame(self.data)\n",
    "            \n",
    "            # Save to parquet\n",
    "            output_file = 'wikipedia_articles.parquet'\n",
    "            df.to_parquet(output_file, engine='pyarrow', compression='snappy')\n",
    "            \n",
    "            self.logger.info(f\"Saved {len(self.data)} articles to {output_file}\")\n",
    "            self.logger.info(f\"DataFrame shape: {df.shape}\")\n",
    "            \n",
    "            # Print summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SCRAPING SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total pages scraped: {len(self.data)}\")\n",
    "            print(f\"Output file: {output_file}\")\n",
    "            print(f\"\\nDataFrame columns: {list(df.columns)}\")\n",
    "            print(f\"\\nFirst 10 titles:\")\n",
    "            for i, title in enumerate(df['title'].head(10), 1):\n",
    "                print(f\"  {i}. {title}\")\n",
    "            print(f\"\\nLast 10 titles:\")\n",
    "            for i, title in enumerate(df['title'].tail(10), 1):\n",
    "                print(f\"  {len(df)-10+i}. {title}\")\n",
    "            print(f\"\\nAverage tokens per article: {df['token_count'].mean():.0f}\")\n",
    "            print(f\"Average text length: {df['text_length'].mean():.0f} characters\")\n",
    "            print(f\"Total tokens across all articles: {df['token_count'].sum()}\")\n",
    "            print(\"=\"*60)\n",
    "        else:\n",
    "            self.logger.warning(\"No data collected!\")\n",
    "\n",
    "\n",
    "def run_spider():\n",
    "    \"\"\"Run the Wikipedia spider\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Wikipedia BFS Crawler\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Starting from: https://en.wikipedia.org/wiki/Madagascar\")\n",
    "    print(\"Target pages: 1000\")\n",
    "    print(\"Strategy: BFS with 10 random links per page\")\n",
    "    print(\"Estimated time: 10-15 minutes\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    process = CrawlerProcess(settings={\n",
    "        'FEEDS': {},\n",
    "        'LOG_LEVEL': 'INFO',\n",
    "    })\n",
    "    \n",
    "    process.crawl(WikipediaSpider)\n",
    "    process.start()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_spider()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defaultenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
